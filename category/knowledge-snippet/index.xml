<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Knowledge snippet | Edo</title>
    <link>https://edoardocostantini.github.io/category/knowledge-snippet/</link>
      <atom:link href="https://edoardocostantini.github.io/category/knowledge-snippet/index.xml" rel="self" type="application/rss+xml" />
    <description>Knowledge snippet</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 06 Sep 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://edoardocostantini.github.io/media/icon_hubc644763d821b4643633e77d54abca59_619947_512x512_fill_lanczos_center_3.png</url>
      <title>Knowledge snippet</title>
      <link>https://edoardocostantini.github.io/category/knowledge-snippet/</link>
    </image>
    
    <item>
      <title>Understanding quantiles</title>
      <link>https://edoardocostantini.github.io/post/quantiles/</link>
      <pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/quantiles/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;quantiles-percentiles-and-quartiles&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Quantiles, percentiles, and quartiles&lt;/h2&gt;
&lt;p&gt;Descriptive statistics are a fundamental tools for a data analysis.
Their purpose is to describe and summarize data.
Quantitative variables such as weight, age, and income can be described by distributions (e.g., normal distribution) with measures of &lt;strong&gt;center&lt;/strong&gt; (the typical value of a variable) and &lt;strong&gt;variability&lt;/strong&gt; (spread around the center).
The mean is a measure of center.
The standard deviation is a measure of variability.&lt;/p&gt;
&lt;p&gt;There are special descriptive statistics that help us describe simultaneously center and spread of a variable’s distribution.
These measures are often known as measures of &lt;strong&gt;positions&lt;/strong&gt;.
In general, they tell us the point of the distribution of a variable at which a given percentage of the data falls below or above that point.
The minimum and maximum values of a variable are measures of positions defining the point at which no data, or all data, fall below it, respectively.
The median is a measure of position that defines the point at which half of the data falls below it (and above it).
The median is a special case of the measure of position called &lt;strong&gt;percentile&lt;/strong&gt;.
The p-th percentile is the point such that p% of the observations fall below or at that point and (100 - p)% fall above it.
The 50-th percentile is the median.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quartiles&lt;/strong&gt; are commonly used percentiles.
The first quartile is the 25-th percentile, the value of the variable leaving to its left 25% of the distribution.
The third quartile is the 75-th percentile, the value of the variable leaving to its left 75% of the distribution.
You can think of quartiles as diving the probability distribution in 4 intervals with equal probabilities (e.g., area under the probability density function).
Similarly, you can think of percentiles as diving the probability distribution in 100 intervals with equal probabilities.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edoardocostantini.github.io/post/quantiles/index_files/figure-html/quartile-pic-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We use the word &lt;strong&gt;quantile&lt;/strong&gt; to describe the general measure of position that divides the probability distribution in intervals with equal probabilities.
Percentiles divide the probability distribution of a variable in 100 intervals.
Deciles divide it in 10 intervals, and quartiles in four.
As such, percentiles, deciles, quartiles, and the median are all special cases of quantiles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR, just give me the code!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set a seed

set.seed(20220906)

# Sample from a normal distribution

age &amp;lt;- rnorm(1e5, mean = 27, sd = 2)

# Define the 1st and 2nd quartile

quartiles &amp;lt;- quantile(age, probs = c(.25, .75))

# Plot density distribution

plot(density(age),
     main = &amp;quot;Quartiles for the probability distribution of age&amp;quot;,
     xlab = NA)

# Costum x ticks
axis(side = 1, at = c(27, round(quartiles, 1)), labels = TRUE)

# Add points for quantiles
points(c(quartiles, median(age)),
        y = rep(0, length(quartiles)+1))
points(c(quartiles, median(age)),
       y = dnorm(c(quartiles, median(age)), mean = mean(age), sd = sd(age)))

# Add segments to devide plot
segments(x0 = quartiles[1], y0 = 0,
         x1 = quartiles[1], y1 = dnorm(quartiles[1],
                                       mean = mean(age), sd = sd(age)))
segments(x0 = median(age), y0 = 0,
         x1 = median(age), y1 = max(dnorm(age,
                                       mean = mean(age), sd = sd(age))))
segments(x0 = quartiles[2], y0 = 0,
         x1 = quartiles[2], y1 = dnorm(quartiles[2],
                                       mean = mean(age), sd = sd(age)))

# Add quartile labels
text(x = quartiles[1],
     y = -.005,
     &amp;quot;1st quartile&amp;quot;)
text(x = quartiles[2],
     y = -.005,
     &amp;quot;3rd quartile&amp;quot;)

# Add percentage under the curve labels
text(x = c(24, 30, 26.3, 27.7),
     y = c(.03, .03, .06, .06),
     &amp;quot;25 %&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding boxplots</title>
      <link>https://edoardocostantini.github.io/post/boxplots/</link>
      <pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/boxplots/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;reading-a-boxplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading a boxplot&lt;/h2&gt;
&lt;p&gt;Boxplots are descriptive tools to visualize the distribution of variables with a focus on their measures of spread and center.
A boxplots report in the same figure the median, the 1st and 3rd quartiles, and indicate possible outliers.&lt;/p&gt;
&lt;p&gt;Imagine wanting to plot the distribution of age in a court of students enrolled in a master program at a university.
The age of the students is likely to be normally distributed around a mean of 26.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up ----------------------------------------------------------------------

# Set seed
set.seed(20220906)

# Generate some age variable for a university master programme
age &amp;lt;- round(rnorm(1e3, mean = 26, sd = 2), 0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can create the boxplot of this age variable in R by using the &lt;code&gt;boxplot()&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Look at the boxplot ---------------------------------------------------------
boxplot(age)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://edoardocostantini.github.io/post/boxplots/index_files/figure-html/boxplot-explained-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The variable age is centered around 26 and 50% of the distribution is located between 25 (1st quartile) and 27 (3rd quartile).
There are 6 values that represent possible outliers (the circles outside the whiskers).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;play-around-with-boxplots&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Play around with boxplots&lt;/h2&gt;
&lt;p&gt;You can compute the statistics used to draw the boxplot explicitly by following this code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute boxplot statistics manually ------------------------------------------

# Compute the median
med &amp;lt;- median(age)

# Compute 1st and 3rd quartiles
qnt &amp;lt;- quantile(age, probs = c(.25, .75))

# Compute interquartile range
IQR &amp;lt;- diff(qnt)[[1]]

# Compute fences/whisker bounds
C &amp;lt;- 1.5 # range multiplier
fences &amp;lt;- c(lwr = qnt[[1]] - C * IQR, upr = qnt[[2]] + C * IQR)

# Put together the boxplot stats
bxstats &amp;lt;- sort(c(med = med, qnt, f = fences))

# Compute boxplot statistics with R function
bxstats_auto &amp;lt;- boxplot.stats(age, coef = C)$stats

# Compare results obtain manually and with the R function
data.frame(manual = bxstats, R.function = bxstats_auto)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       manual R.function
## f.lwr     22         22
## 25%       25         25
## med       26         26
## 75%       27         27
## f.upr     30         30&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can visualize the impact of different choices for the range multiplier &lt;code&gt;C&lt;/code&gt;.
In the following pictures, you can see that a larger &lt;code&gt;C&lt;/code&gt; is less restrictive in which values are considered outliers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://edoardocostantini.github.io/post/boxplots/index_files/figure-html/boxplot-c-1.png&#34; width=&#34;1344&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR, just give me the code!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set up ----------------------------------------------------------------------

# Set seed
set.seed(20220906)

# Generate some age variable for a university master programme
age &amp;lt;- round(rnorm(1e3, mean = 26, sd = 2), 0)

# Look at the boxplot ---------------------------------------------------------
boxplot(age)

# Boxplot with explanation
C &amp;lt;- 1.5 # range multiplier
boxplot(age, range = C)

# Add arrows pointings to statistics
arrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,
       x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,
       length = 0.1)

# Add labels of statistics
text(x = rep(.66, 5),
     y = boxplot.stats(age, coef = C)$stats,
     labels = c(&amp;quot;lower whisker&amp;quot;,
                &amp;quot;1st quartile&amp;quot;,
                &amp;quot;median&amp;quot;,
                &amp;quot;3rd quartile&amp;quot;,
                &amp;quot;upper whisker&amp;quot;),
     adj = 1)

# Add y axis labels
axis(side = 2, at = boxplot.stats(age, coef = C)$stats[c(1, 3, 4)], labels = TRUE)

# Compute boxplot statistics manually ------------------------------------------

# Compute the median
med &amp;lt;- median(age)

# Compute 1st and 3rd quartiles
qnt &amp;lt;- quantile(age, probs = c(.25, .75))

# Compute interquartile range
IQR &amp;lt;- diff(qnt)[[1]]

# Compute fences/whisker bounds
C &amp;lt;- 1.5 # range multiplier
fences &amp;lt;- c(lwr = qnt[[1]] - C * IQR, upr = qnt[[2]] + C * IQR)

# Put together the boxplot stats
bxstats &amp;lt;- sort(c(med = med, qnt, f = fences))

# Compute boxplot statistics with R function
bxstats_auto &amp;lt;- boxplot.stats(age, coef = C)$stats

# Compare results obtain manually and with the R function
data.frame(manual = bxstats, R.function = bxstats_auto)

# Visualize the effect of different C -----------------------------------------

# Allow two plots one next to the other
par(mfrow = c(1, 2))

# Plot C = 1.5 and 3
lapply(c(1.5, 3.0), FUN = function (x){
  C &amp;lt;- x
  boxplot(age, range = C, main = paste0(&amp;quot;C = &amp;quot;, C))

  # Add arrows pointings to statistics
  arrows(x0 = .69, y0 = boxplot.stats(age, coef = C)$stats,
         x1 = c(.875, rep(.765, 3), .875), y1 = boxplot.stats(age, coef = C)$stats,
         length = 0.1)
  # Add labels of statistics
  text(x = rep(.66, 5),
       y = boxplot.stats(age, coef = C)$stats,
       labels = c(paste(ifelse(C == 1.5, &amp;quot;inner&amp;quot;, &amp;quot;outer&amp;quot;), &amp;quot;fence \n lower bound&amp;quot;),
                  &amp;quot;1st quartile&amp;quot;,
                  &amp;quot;median&amp;quot;,
                  &amp;quot;3rd quartile&amp;quot;,
                  paste(ifelse(C == 1.5, &amp;quot;inner&amp;quot;, &amp;quot;outer&amp;quot;), &amp;quot;fence \n upper bound&amp;quot;)),
       adj = 1)
})&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Understanding the residual standard error</title>
      <link>https://edoardocostantini.github.io/post/residual-standard-error/</link>
      <pubDate>Wed, 22 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://edoardocostantini.github.io/post/residual-standard-error/</guid>
      <description>

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;The residual standard error is a measure of fit for linear regression models.
Conceptually, it can be thought of as the variability of the prediction error for a linear model.
It is usually calculated as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
SE_{resid} = \sqrt{ \frac{ \sum^{n}_{i = 1}(y_i - \hat{y}_i)^2 }{df_{resid}} }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the sample size&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is the number of parameters to estimate in the model&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(-1\)&lt;/span&gt; is the degree of freedom lost to estimate the intercept&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_i\)&lt;/span&gt; is the fitted &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; value for the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;-th individual&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(df_{resid}\)&lt;/span&gt; is the degrees of freedom of the residuals (&lt;span class=&#34;math inline&#34;&gt;\(n - k - 1\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The smaller the residual standard error, the better the model fits the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;learn-by-coding&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learn by coding&lt;/h2&gt;
&lt;p&gt;We can compute the residual standard error manually after estimating a linear model in R.
To get a better grasp of the residual standard error, let’s start by regressing the miles per gallon (mpg) on the number of cylinders (cyl), horsepower (hp), and weight (wt) of cars from the standard &lt;code&gt;mtcars&lt;/code&gt; R dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit a linear model -----------------------------------------------------------

  lm_fit &amp;lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can compute the residual standard error following the formula described above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compute the residual standard error manually ---------------------------------

  # Define elements of the formula
  n &amp;lt;- nrow(mtcars) # sample size
  k &amp;lt;- 3            # number of parameters (regression coefficients)
  yhat &amp;lt;- fitted(lm_fit) # fitted y values
  y &amp;lt;- mtcars$mpg

  # Compute rse
  rse &amp;lt;- sqrt(sum((y - yhat)^2) / (n - k - 1))

  # Print rse
  rse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.511548&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also extract it directly from any &lt;code&gt;lm&lt;/code&gt; object:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# residual standard error from lm output ---------------------------------------

  # Use the sigma function to extract it from an lm object
  sigma(lm_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.511548&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # Compare with the manual computation
  sigma(lm_fit) - rse&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tldr-just-give-me-the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;TL;DR, just give me the code!&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit a linear model -----------------------------------------------------------

  lm_fit &amp;lt;- lm(mpg ~ cyl + hp + wt, data = mtcars)

# Compute the residual standard error manually ---------------------------------

  # Define elements of the formula
  n &amp;lt;- nrow(mtcars) # sample size
  k &amp;lt;- 3            # number of parameters (regression coefficients)
  yhat &amp;lt;- fitted(lm_fit) # fitted y values
  y &amp;lt;- mtcars$mpg

  # Compute rse
  rse &amp;lt;- sqrt(sum((y - yhat)^2) / (n - k - 1))

  # Print rse
  rse

# residual standard error from lm output ---------------------------------------

  # Use the sigma function to extract it from an lm object
  sigma(lm_fit)

  # Compare with the manual computation
  sigma(lm_fit) - rse&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-resources&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.statology.org/how-to-interpret-residual-standard-error/&#34;&gt;Statology: How to Interpret Residual Standard Error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.statology.org/residual-standard-error-r/&#34;&gt;Statology: How to Calculate Residual Standard Error in R&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
