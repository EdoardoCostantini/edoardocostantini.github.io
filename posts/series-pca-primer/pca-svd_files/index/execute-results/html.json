{
  "hash": "a0dcbe0742418af0e641f0dd1730e575",
  "result": {
    "markdown": "---\ntitle: Principal Component Analysis and SVD\nauthor: Edoardo Costantini\ndate: '2022-05-13'\nslug: pcasvd\ncategories: [\"PCA\"]\nbibliography: ../../../resources/bibshelf.bib\n---\n\n\n# Introduction\n\nPrincipal Component Analysis (PCA) is a technique that finds a low-dimensional representation of a large set of variables contained in an $n \\times p$ data matrix $\\mathbf{X}$ with minimal loss of information.\nWe refer to this low-dimensional representation as the $n \\times r$ matrix $\\mathbf{Z}$, where $r < p$.\n\nThe columns of $\\mathbf{Z}$ are called principal components (PCs) of $\\mathbf{X}$.\nWe follow the common practice of assuming that the columns of $\\mathbf{X}$ are mean-centered and scaled to have a variance of 1.\nThe first PC of $\\mathbf{X}$ is the linear combination of the columns of $\\mathbf{X}$ with the largest variance:\n$$\n    \\mathbf{z}_1 = \\lambda_{11} \\mathbf{x}_1 + \\lambda_{12} \\mathbf{x}_2 + \\dots + \\lambda_{1p} \\mathbf{x}_p = \\mathbf{X} \\mathbf{\\lambda}_1\n$$\nwith $\\mathbf{\\lambda}_1$ being the $1 \\times p$ vector of coefficients $\\lambda_{11}, \\dots, \\lambda_{1p}$.\nThe second principal component ($\\mathbf{z}_2$) is defined by finding the vector of coefficients $\\mathbf{\\lambda}_2$ giving the linear combination of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$ with maximal variance out of all the linear combinations that are uncorrelated with $\\mathbf{z}_1$.\nEvery subsequent column of $\\mathbf{Z}$ can be understood in the same way.\nAs a result, the PCs are independent by definition and every subsequent PC has less variance than the preceding one.\nWe can write the relationship between all the PCs and $\\mathbf{X}$ in matrix notation:\n\\begin{equation} \\label{eq:PCAmatnot}\n    \\mathbf{Z} = \\mathbf{X} \\mathbf{\\Lambda}\n\\end{equation}\nwhere $\\mathbf{\\Lambda}$ is a $p \\times r$ matrix of weights, with columns $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_q$.\nPCA can be thought of as the process of projecting the original data from a $p$-dimensional space to a lower $q$-dimensional space.\nThe coefficient vectors $\\mathbf{\\lambda}_1, \\dots, \\mathbf{\\lambda}_r$ define the directions in which we are projecting the $n$ observations of $\\mathbf{x}_1, \\dots, \\mathbf{x}_p$.\nThe projected values are the principal component scores $\\mathbf{Z}$.\n\nThe goal of PCA is to find the values of $\\mathbf{\\Lambda}$ that maximize the variance of the columns of $\\mathbf{Z}$.\nOne way to find the PCA solution for $\\mathbf{\\Lambda}$ is by taking the truncated [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition) (SVD) of $\\mathbf{X}$:\n\n\\begin{equation} \\label{eq:SVD}\n    \\mathbf{X} = \\mathbf{UDV}'\n\\end{equation}\n\nwhere:\n\n- $\\mathbf{D}$ is the $r \\times r$ diagonal matrix with elements equal to the square root of the non-zero eigenvalues of $\\mathbf{XX}^T$ and $\\mathbf{X}^T\\mathbf{X}$;\n- $\\mathbf{U}$ is the $n \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{XX}^T$ such that $\\mathbf{U}^T\\mathbf{U=I}$\n- $\\mathbf{V}$ is the $p \\times r$ matrix of the first $r$ eigenvectors of $\\mathbf{X}^T\\mathbf{NXM}$ such that $\\mathbf{V}^T\\mathbf{V=I}$;\n\nThe PCs scores are given by the $n \\times r$ matrix $\\mathbf{UD}$, and the weights $\\mathbf{\\Lambda}$ are given by the $p \\times r$ matrix $\\mathbf{V}$ [@jolliffe:2002 p45].\n\n## SVD solution to PCA\n\n## Eigen-decomposition solution to PCA\n\n# Learn by coding\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsome <- 1\n```\n:::\n\n\n# TL;DR, just give me the code!\n\n::: {.cell}\n\n```{.r .cell-code}\nsome <- 1\n```\n:::\n\n\n# References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}